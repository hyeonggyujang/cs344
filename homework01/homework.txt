    Modelling human cognitive processes by making an AI to introspect generally would not be a good idea. Introspection works only if there are unconscious internal mental processes. Introspection reveals the unconscious idea or feeling by allowing the observing subject--which in this case, the self--to delineate the a series of cause-and-effect relationships of consciuos thoughts that eventually cause the unconsciuos idea or feeling. Machines, by default, don't have unconsciuosness, which makes introspection impossible.
    It will make sense if we train an AI machine with lots of data on human concsiousness and some algorithms to process those conscious ideas and feelings, then we might be able to approximate general cognitive processes. I say "approximate," because there could be innumerable causes for an idea or feeling depending on an individual. Nothing, including all humans, can exactly model a human consciousness that's universal for all population.